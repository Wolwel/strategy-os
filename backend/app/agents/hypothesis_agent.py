from app.services.llm_service import get_llm_response
import json

async def decompose_hypotheses(user_request: str, framing_context: str) -> dict:
    """
    Hypothesis Decomposer Agent.
    –†–æ–∑–∫–ª–∞–¥–∞—î –∑–∞–¥–∞—á—É –Ω–∞ –ø–µ—Ä–µ–≤—ñ—Ä—é–≤–∞–Ω—ñ –≥—ñ–ø–æ—Ç–µ–∑–∏, –ø—Ä—ñ–æ—Ä–∏—Ç–µ–∑—É—î –∑–∞ Impact/–†–∏–∑–∏–∫.
    """
    print("üî¨ [Hypothesis Agent] Decomposing into testable hypotheses...")

    prompt = f"""
    –¢–∏ - Hypothesis Decomposer Agent. –¢–≤–æ—è –∑–∞–¥–∞—á–∞ - —Ä–æ–∑–∫–ª–∞—Å—Ç–∏ –±—ñ–∑–Ω–µ—Å-—ñ–¥–µ—é –Ω–∞ –ö–û–ù–ö–†–ï–¢–ù–Ü –ø–µ—Ä–µ–≤—ñ—Ä—é–≤–∞–Ω—ñ –≥—ñ–ø–æ—Ç–µ–∑–∏.

    –í—Ö—ñ–¥–Ω—ñ –¥–∞–Ω—ñ:
    - –ó–∞–ø–∏—Ç –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞: "{user_request}"
    - –ö–æ–Ω—Ç–µ–∫—Å—Ç (Problem Framing): {framing_context}

    ‚ö†Ô∏è –ü–†–ê–í–ò–õ–ê:
    1. –ö–æ–∂–Ω–∞ –≥—ñ–ø–æ—Ç–µ–∑–∞ –º–∞—î –±—É—Ç–∏ –¢–ï–°–¢–û–í–ê–ù–û–Æ (–º–æ–∂–Ω–∞ –¥–æ–≤–µ—Å—Ç–∏ –∞–±–æ —Å–ø—Ä–æ—Å—Ç—É–≤–∞—Ç–∏).
    2. –ü—Ä—ñ–æ—Ä–∏—Ç–µ–∑—É–π –∑–∞ Impact (–≤–ø–ª–∏–≤ –Ω–∞ —É—Å–ø—ñ—Ö) √ó Uncertainty (–Ω–µ–≤–∏–∑–Ω–∞—á–µ–Ω—ñ—Å—Ç—å).
    3. –í–∫–∞–∂–∏, —è–∫–∏–π –∞–≥–µ–Ω—Ç –º–∞—î –ø–µ—Ä–µ–≤—ñ—Ä–∏—Ç–∏ –∫–æ–∂–Ω—É –≥—ñ–ø–æ—Ç–µ–∑—É.
    4. –ë—É–¥—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω–∏–º, —É–Ω–∏–∫–∞–π –∑–∞–≥–∞–ª—å–Ω–∏—Ö —Ñ—Ä–∞–∑.

    –§–û–†–ú–ê–¢ –í–Ü–î–ü–û–í–Ü–î–Ü (JSON):
    {{
        "hypotheses": [
            {{
                "id": 1,
                "text": "–ö–æ–Ω–∫—Ä–µ—Ç–Ω–∞ –≥—ñ–ø–æ—Ç–µ–∑–∞ –¥–ª—è –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏",
                "category": "market|product|finance|competition|regulation",
                "impact": "high|medium|low",
                "uncertainty": "high|medium|low",
                "priority": 1,
                "agent_to_verify": "market_agent|competitor_agent|finance_agent|advocate_agent",
                "how_to_test": "–ö–æ—Ä–æ—Ç–∫–∏–π –æ–ø–∏—Å —è–∫ –ø–µ—Ä–µ–≤—ñ—Ä–∏—Ç–∏"
            }}
        ],
        "critical_assumptions": [
            "–ü—Ä–∏–ø—É—â–µ–Ω–Ω—è 1, —è–∫–µ –º–∞—î –±—É—Ç–∏ –ø—Ä–∞–≤–¥–∏–≤–∏–º –¥–ª—è —É—Å–ø—ñ—Ö—É",
            "–ü—Ä–∏–ø—É—â–µ–Ω–Ω—è 2"
        ],
        "recommended_agents": ["market", "competitors", "finance", "risks", "frameworks"]
    }}

    –ü–æ–≤–µ—Ä–Ω–∏ 4-7 –≥—ñ–ø–æ—Ç–µ–∑, –≤—ñ–¥—Å–æ—Ä—Ç–æ–≤–∞–Ω–∏—Ö –∑–∞ –ø—Ä—ñ–æ—Ä–∏—Ç–µ—Ç–æ–º (1 = –Ω–∞–π–≤–∞–∂–ª–∏–≤—ñ—à–∞).
    –í—ñ–¥–ø–æ–≤—ñ–¥—å –¢–Ü–õ–¨–ö–ò JSON, –±–µ–∑ markdown, –±–µ–∑ –ø–æ—è—Å–Ω–µ–Ω—å.
    """

    response_text = await get_llm_response(prompt, temperature=0.3, agent="hypothesis_agent")
    
    # –ß–∏—Å—Ç–∏–º–æ –≤—ñ–¥–ø–æ–≤—ñ–¥—å
    cleaned_text = response_text.replace("```json", "").replace("```", "").strip()
    
    try:
        result = json.loads(cleaned_text)
        return result
    except json.JSONDecodeError:
        print("‚ö†Ô∏è Hypothesis Agent failed to produce JSON, returning default structure.")
        return {
            "hypotheses": [
                {
                    "id": 1,
                    "text": "–†–∏–Ω–æ–∫ –º–∞—î –¥–æ—Å—Ç–∞—Ç–Ω—ñ–π –ø–æ–ø–∏—Ç –¥–ª—è —Ü—å–æ–≥–æ –ø—Ä–æ–¥—É–∫—Ç—É",
                    "category": "market",
                    "impact": "high",
                    "uncertainty": "high",
                    "priority": 1,
                    "agent_to_verify": "market_agent",
                    "how_to_test": "–ê–Ω–∞–ª—ñ–∑ TAM/SAM/SOM —Ç–∞ —Ç—Ä–µ–Ω–¥—ñ–≤"
                },
                {
                    "id": 2,
                    "text": "–ö–æ–Ω–∫—É—Ä–µ–Ω—Ü—ñ—è –Ω–µ —î –∫—Ä–∏—Ç–∏—á–Ω–∏–º –±–∞—Ä'—î—Ä–æ–º",
                    "category": "competition",
                    "impact": "high",
                    "uncertainty": "medium",
                    "priority": 2,
                    "agent_to_verify": "competitor_agent",
                    "how_to_test": "–ú–∞–ø—ñ–Ω–≥ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç—ñ–≤ —Ç–∞ —ó—Ö –ø–æ–∑–∏—Ü—ñ–π"
                },
                {
                    "id": 3,
                    "text": "Unit Economics –¥–æ–∑–≤–æ–ª—è—î –¥–æ—Å—è–≥—Ç–∏ –ø—Ä–∏–±—É—Ç–∫–æ–≤–æ—Å—Ç—ñ",
                    "category": "finance",
                    "impact": "high",
                    "uncertainty": "high",
                    "priority": 3,
                    "agent_to_verify": "finance_agent",
                    "how_to_test": "–†–æ–∑—Ä–∞—Ö—É–Ω–æ–∫ –º–∞—Ä–∂—ñ, CAC, LTV"
                }
            ],
            "critical_assumptions": [
                "–ö–ª—ñ—î–Ω—Ç–∏ –≥–æ—Ç–æ–≤—ñ –ø–ª–∞—Ç–∏—Ç–∏ –∑–∞ —Ü–µ–π –ø—Ä–æ–¥—É–∫—Ç",
                "–ö–æ–º–∞–Ω–¥–∞ –º–∞—î –Ω–µ–æ–±—Ö—ñ–¥–Ω—ñ –∫–æ–º–ø–µ—Ç–µ–Ω—Ü—ñ—ó"
            ],
            "recommended_agents": ["market", "competitors", "finance", "risks", "frameworks"]
        }


def format_hypotheses_for_report(hypotheses_data: dict) -> str:
    """
    –§–æ—Ä–º–∞—Ç—É—î –≥—ñ–ø–æ—Ç–µ–∑–∏ —É Markdown –¥–ª—è –≤–∫–ª—é—á–µ–Ω–Ω—è —É —Ñ—ñ–Ω–∞–ª—å–Ω–∏–π –∑–≤—ñ—Ç.
    """
    if not hypotheses_data or "hypotheses" not in hypotheses_data:
        return "N/A"
    
    report = "### üî¨ –ö–ª—é—á–æ–≤—ñ –≥—ñ–ø–æ—Ç–µ–∑–∏ –¥–ª—è –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏\n\n"
    report += "| # | –ì—ñ–ø–æ—Ç–µ–∑–∞ | Impact | –ù–µ–≤–∏–∑–Ω–∞—á–µ–Ω—ñ—Å—Ç—å | –Ø–∫ –ø–µ—Ä–µ–≤—ñ—Ä–∏—Ç–∏ |\n"
    report += "|---|----------|--------|----------------|---------------|\n"
    
    for h in hypotheses_data.get("hypotheses", []):
        impact_emoji = {"high": "üî¥", "medium": "üü°", "low": "üü¢"}.get(h.get("impact", ""), "")
        uncert_emoji = {"high": "‚ùì‚ùì", "medium": "‚ùì", "low": "‚úì"}.get(h.get("uncertainty", ""), "")
        report += f"| {h.get('priority', '-')} | {h.get('text', '')} | {impact_emoji} {h.get('impact', '')} | {uncert_emoji} {h.get('uncertainty', '')} | {h.get('how_to_test', '')} |\n"
    
    if hypotheses_data.get("critical_assumptions"):
        report += "\n### ‚ö†Ô∏è –ö—Ä–∏—Ç–∏—á–Ω—ñ –ø—Ä–∏–ø—É—â–µ–Ω–Ω—è\n"
        for assumption in hypotheses_data["critical_assumptions"]:
            report += f"- {assumption}\n"
    
    return report
